{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb6EAkjnWzwq"
      },
      "source": [
        "# **PART-1: Dense Network**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/\n"
      ],
      "metadata": {
        "id": "R_NMaxHhDpNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n"
      ],
      "metadata": {
        "id": "v6bh__BwE-_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a-C7sUxWvKn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from torch.utils.tensorboard import SummaryWriter  # Corrected import statement\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the data from the file\n",
        "data = np.load('emnist_letters.npz')\n",
        "\n",
        "# Access the arrays containing images and labels\n",
        "train_images = data['train_images']\n",
        "train_labels = data['train_labels']\n",
        "val_images = data['validate_images']\n",
        "val_labels = data['validate_labels']\n",
        "test_images = data['test_images']\n",
        "test_labels = data['test_labels']\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = TensorDataset(torch.tensor(train_images), torch.tensor(train_labels))\n",
        "val_dataset = TensorDataset(torch.tensor(val_images), torch.tensor(val_labels))\n",
        "test_dataset = TensorDataset(torch.tensor(test_images), torch.tensor(test_labels))\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Model Definition\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        # self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28 * 28, 300, dtype=torch.float32)  # One neuron per feature- each input pixel is a feature, 28 * 28  pixel images, therefore 28 * 28  neurons\n",
        "        self.fc2 = nn.Linear(300,150, dtype=torch.float32)  # arbitrarily selecting nerons for each layer, should adjust during model tuning-\n",
        "        self.fc3 = nn.Linear(150, 50, dtype=torch.float32)\n",
        "        self.fc4 = nn.Linear(50, 27, dtype=torch.float32)  # 27 Classes present in dataset\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.flatten(x)\n",
        "        # Cast input data to torch.float32\n",
        "        x = x.to(torch.float32)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = NeuralNetwork()\n",
        "print(model)\n",
        "\n",
        "# Inspect a sample of the data\n",
        "for data, target in train_loader:\n",
        "    print(\"Target shape:\", target.shape)\n",
        "    print(\"Target dataset:\", target)\n",
        "    break"
      ],
      "metadata": {
        "id": "VWNBfN-6e3N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Create a directory for TensorBoard logs\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Create a SummaryWriter for TensorBoard\n",
        "writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "def train(model, train_loader, optimizer, criterion, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0  # Track epoch loss\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "\n",
        "            # Convert one-hot encoded target to class labels (1D tensor)\n",
        "            target_labels = torch.nonzero(target, as_tuple=True)[1]\n",
        "\n",
        "            # Calculate loss using class labels\n",
        "            loss = criterion(output, target_labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()  # Accumulate batch loss\n",
        "\n",
        "        # Print epoch results\n",
        "        print('Epoch {} - Loss: {:.6f}'.format(epoch, epoch_loss / len(train_loader)))\n",
        "        # Log the training loss to TensorBoard\n",
        "        writer.add_scalar('Loss/train', epoch_loss / len(train_loader), epoch)\n",
        "\n"
      ],
      "metadata": {
        "id": "zKdpOInMe3So"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            # Convert one-hot encoded target to class labels (1D tensor)\n",
        "            target_labels = torch.nonzero(target, as_tuple=True)[1]\n",
        "            test_loss += criterion(output, target_labels).item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target_labels.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n"
      ],
      "metadata": {
        "id": "NRj2hmt-e3iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, train_loader, optimizer, criterion, epochs=10)\n",
        "# Close the SummaryWriter\n",
        "writer.close()\n",
        "test(model, test_loader)"
      ],
      "metadata": {
        "id": "1nJGUcMpe3tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graphing the data from the Dense network\n",
        "\n",
        "First, lets generate confusion matrix of the dense network's classifications\n",
        "\n",
        "We'll start by calculating the number of true and false positives"
      ],
      "metadata": {
        "id": "JBs08iQoG8SJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate predictions for the test set\n",
        "def generate_predictions(model, test_loader):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            all_predictions.extend(output.argmax(dim=1).cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "    return np.array(all_predictions), np.array(all_targets)\n",
        "def compute_tp_fp(predictions, targets, class_label):\n",
        "    # Convert one-hot encoded targets to class labels\n",
        "    target_labels = np.argmax(targets, axis=1)\n",
        "\n",
        "    # Compute True Positives (TP) and False Positives (FP) for the specified class label\n",
        "    tp = np.sum((predictions == class_label) & (target_labels == class_label))\n",
        "    fp = np.sum((predictions == class_label) & (target_labels != class_label))\n",
        "\n",
        "    return tp, fp\n",
        "\n",
        "# Generate predictions for the test set\n",
        "test_predictions, test_targets = generate_predictions(model, test_loader)\n",
        "\n",
        "# Compute TP and FP for each class\n",
        "for class_label in range(27):\n",
        "    tp, fp = compute_tp_fp(test_predictions, test_targets, class_label)\n",
        "    print(f\"Class {class_label}: TP={tp}, FP={fp}\")"
      ],
      "metadata": {
        "id": "HeQwAGiAG80Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Confusion Matrix"
      ],
      "metadata": {
        "id": "Vnkxlfh4HsGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Generate predictions for the test set\n",
        "test_predictions, test_targets = generate_predictions(model, test_loader)\n",
        "\n",
        "# Convert one-hot encoded targets to class labels\n",
        "test_targets_single = np.argmax(test_targets, axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(test_targets_single, test_predictions)\n",
        "\n",
        "# Generate labels for the letters using Unicode\n",
        "letter_labels = [chr(ord('A') + i) for i in range(26)]\n",
        "\n",
        "# Plot confusion matrix as heatmap with x-axis on top and \"plasma\" colormap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"plasma\", xticklabels=letter_labels, yticklabels=letter_labels)\n",
        "plt.xlabel(\"Predicted Letter\")\n",
        "plt.ylabel(\"True Letter\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oS4JcbL_HsO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "keEvx_NsEYV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Performance Comparison between Dense Network and OPIUM based Classifier\n",
        "\n",
        "The Dense Network used 3 hidden layers but the OPIUM based classifier used 10,000 hidden layers. Even with the huge increase in the hidden layers the accuracy of the OPIUM based classifier on the letters dataset remained at (85.15% Â± 0.12%), while the dense network had a much better accuracy of 91%. In comparison with the OPIUM based classifier the dense network is more compact and runs more efficiently."
      ],
      "metadata": {
        "id": "1iVVtyRmls2n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p3zQAQgWv8L"
      },
      "source": [
        "# **PART-2: Convolutional Network**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "import datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Activation, BatchNormalization, MaxPooling2D, Dropout,Flatten, Dense\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "cD3_vub5aGeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mci19SwKUiOO"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/\n",
        "# Load the data from the file\n",
        "data = np.load('emnist_letters.npz')\n",
        "\n",
        "# Access the arrays containing images and labels\n",
        "train_images = data['train_images']\n",
        "train_labels = data['train_labels']\n",
        "validate_images = data['validate_images']\n",
        "validate_labels = data['validate_labels']\n",
        "test_images = data['test_images']\n",
        "test_labels = data['test_labels']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the log directory for TensorBoard\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Define TensorBoard callback\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "bglcjFJqZ_CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuzDNs34XfA-"
      },
      "outputs": [],
      "source": [
        "num_train_images = train_images.shape[0]\n",
        "print(\"Number of images in the training dataset:\", num_train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOi9MX3IXvau"
      },
      "outputs": [],
      "source": [
        "# Count occurrences of each label\n",
        "label_counts = np.sum(train_labels, axis=0)\n",
        "\n",
        "# Plot the distribution of labels\n",
        "plt.bar(range(len(label_counts)), label_counts)\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Labels in Training Dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7ysRvk-bA2t"
      },
      "outputs": [],
      "source": [
        "print(train_images.shape)\n",
        "print(validate_images.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGqN2Lw_Yq8l"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRmb8lO1YJDx"
      },
      "outputs": [],
      "source": [
        "# Define the strategy\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "with strategy.scope():\n",
        "    model = Sequential()\n",
        "\n",
        "    # Reshape the input images to their original 28x28 shape (assuming original shape was 28x28)\n",
        "    model.add(tf.keras.layers.Reshape((28, 28, 1), input_shape=(784,)))\n",
        "\n",
        "    # Feature Learning Layers\n",
        "    model.add(Conv2D(32,                  # Number of filters/Kernels\n",
        "                     (3,3),               # Size of kernels (3x3 matrix)\n",
        "                     strides = 1,         # Step size for sliding the kernel across the input (1 pixel at a time).\n",
        "                     padding = 'same'    # 'Same' ensures that the output feature map has the same dimensions as the input by padding zeros around the input.\n",
        "                    ))\n",
        "    model.add(Activation('relu'))# Activation function\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Conv2D(64, (5,5), padding = 'same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Conv2D(128, (3,3), padding = 'same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # Flattening tensors\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Fully-Connected Layers\n",
        "    model.add(Dense(2048))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # Output Layer\n",
        "    model.add(Dense(27, activation = 'softmax')) # Classification layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs_W8X5TZqxC"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.RMSprop(0.0001), # 1e-4\n",
        "              loss = 'categorical_crossentropy', # Ideal for multiclass tasks\n",
        "              metrics = ['accuracy']) # Evaluation metric\n",
        "\n",
        "# Defining an Early Stopping and Model Checkpoints\n",
        "early_stopping = EarlyStopping(monitor = 'val_accuracy',\n",
        "                              patience = 5, mode = 'max',\n",
        "                              restore_best_weights = True)\n",
        "\n",
        "checkpoint = ModelCheckpoint('best_model.h5',\n",
        "                            monitor = 'val_accuracy',\n",
        "                            save_best_only = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05J6iXYTaPKp"
      },
      "outputs": [],
      "source": [
        "# Define the number of epochs\n",
        "num_epochs = 50\n",
        "\n",
        "# Fit the model to the training data\n",
        "history = model.fit(train_images, train_labels,\n",
        "                    epochs=num_epochs,\n",
        "                    validation_data=(validate_images, validate_labels),\n",
        "                    callbacks=[early_stopping, checkpoint, tensorboard_callback])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "v2xgiZrJVjES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pvTxTaT_Umy"
      },
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "best_model = load_model('best_model.h5')\n",
        "\n",
        "# Evaluate the best model on test data\n",
        "test_loss, test_accuracy = best_model.evaluate(test_images, test_labels)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate True and False Positives for Convolutional Network"
      ],
      "metadata": {
        "id": "IReMLh1iHao4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the best model\n",
        "best_model = load_model('best_model.h5')\n",
        "\n",
        "# Generate predictions for the test set\n",
        "def generate_predictions(model, test_images):\n",
        "    predictions = model.predict(test_images)\n",
        "    return np.argmax(predictions, axis=1)\n",
        "\n",
        "# Load the test data\n",
        "test_images = data['test_images']\n",
        "test_labels = data['test_labels']\n",
        "\n",
        "# Flatten the test labels\n",
        "test_labels_flat = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Compute TP and FP for each class\n",
        "def compute_tp_fp(predictions, targets, class_label):\n",
        "    # Compute True Positives (TP) and False Positives (FP) for the specified class label\n",
        "    tp = np.sum((predictions == class_label) & (targets == class_label))\n",
        "    fp = np.sum((predictions == class_label) & (targets != class_label))\n",
        "    return tp, fp\n",
        "\n",
        "# Generate predictions for the test set\n",
        "test_predictions = generate_predictions(best_model, test_images)\n",
        "\n",
        "# Compute TP and FP for each class\n",
        "for class_label in range(27):\n",
        "    tp, fp = compute_tp_fp(test_predictions, test_labels_flat, class_label)\n",
        "    print(f\"Class {class_label}: TP={tp}, FP={fp}\")\n"
      ],
      "metadata": {
        "id": "xVGFnXSGHa3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Confusion Matrix"
      ],
      "metadata": {
        "id": "g-wwsrUqH4GU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define class labels (assuming class labels are represented as integers from 0 to 25, corresponding to letters A to Z)\n",
        "class_labels = range(26)\n",
        "letter_labels = [chr(ord('A') + i) for i in class_labels]\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(test_labels_flat, test_predictions)\n",
        "\n",
        "# Plot confusion matrix as heatmap with \"viridis\" colormap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"viridis\", xticklabels=letter_labels, yticklabels=letter_labels)\n",
        "plt.xlabel(\"Predicted Letter\")\n",
        "plt.ylabel(\"True Letter\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pwTjxt1vH4X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Performance Comparison between Dense Network and CNN\n",
        "\n",
        "The dense network performed at 91% accuracy while the convolution network performed a bit better at 94%. The models both struggled in the same areas and both had most of their misidentifications in the same place. Similar letters were frequently misidentified as each other- for example both of the networks had the most issues misidentifying the letter I as the letter L, and vice versa. The second highest misidentifications were Q and G. Overall though, the Convolution network was more consistent in identifying letters, with far more pairs of letters at 0 total misidentifications.\n"
      ],
      "metadata": {
        "id": "Rx1DWxkLNvgf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HeLQNTP98Gd"
      },
      "source": [
        "# **Part-3: GAN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB-sW7fI98aa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# Load the data from the file\n",
        "\n",
        "data = np.load('emnist_letters.npz')\n",
        "\n",
        "# Access the arrays containing images and labels\n",
        "train_images = data['train_images']\n",
        "train_labels = data['train_labels']\n",
        "validate_images = data['validate_images']\n",
        "validate_labels = data['validate_labels']\n",
        "test_images = data['test_images']\n",
        "test_labels = data['test_labels']\n",
        "\n",
        "\n",
        "# Concatenate images and labels arrays\n",
        "all_images = np.concatenate([train_images, validate_images, test_images], axis=0)\n",
        "all_labels = np.concatenate([train_labels, validate_labels, test_labels], axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqSiQywk-Bbf"
      },
      "outputs": [],
      "source": [
        "# Number of workers for dataloader\n",
        "workers = 2\n",
        "\n",
        "# Batch size during training\n",
        "batch_size = 128\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this\n",
        "#   size using a transformer.\n",
        "image_size = 64\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 1\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 28\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 28\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 50\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.0002\n",
        "\n",
        "# Beta1 hyperparameter for Adam optimizers\n",
        "beta1 = 0.5\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtQHFbyZVbVY"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx].reshape(28, 28)  # Reshape flattened image to 2D\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Transform for image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create custom dataset instances\n",
        "train_dataset = CustomDataset(all_images, all_labels, transform=transform)\n",
        "\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO6o0rrY-B-e"
      },
      "outputs": [],
      "source": [
        "# Decide which device we want to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "real_batch = next(iter(train_dataloader))\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVWHXBGAbt8p"
      },
      "outputs": [],
      "source": [
        "# custom weights initialization called on ``netG`` and ``netD``\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A35T3_fEeUxv"
      },
      "outputs": [],
      "source": [
        "# Generator Code\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. ``(ngf*8) x 4 x 4``\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. ``(ngf*4) x 8 x 8``\n",
        "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. ``(ngf*2) x 16 x 16``\n",
        "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. ``(ngf) x 32 x 32``\n",
        "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. ``(nc) x 64 x 64``\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFRnpLDmebUf"
      },
      "outputs": [],
      "source": [
        "# Create the generator\n",
        "netG = Generator(ngpu).to(device)\n",
        "\n",
        "# Handle multi-GPU if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
        "\n",
        "# Apply the ``weights_init`` function to randomly initialize all weights\n",
        "#  to ``mean=0``, ``stdev=0.02``.\n",
        "netG.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(netG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJAJjWmleuZp"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is ``(nc) x 64 x 64``\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf) x 32 x 32``\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf*2) x 16 x 16``\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf*4) x 8 x 8``\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf*8) x 4 x 4``\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duiQ6CzBezBR"
      },
      "outputs": [],
      "source": [
        "# Create the Discriminator\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "\n",
        "# Handle multi-GPU if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
        "\n",
        "# Apply the ``weights_init`` function to randomly initialize all weights\n",
        "# like this: ``to mean=0, stdev=0.2``.\n",
        "netD.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(netD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7098XbUUfzvI"
      },
      "outputs": [],
      "source": [
        "# Initialize the ``BCELoss`` function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-lRCFkOj5cC"
      },
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "\n",
        "# Lists to keep track of progress\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    # For each batch in the dataloader\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        ## Train with all-real batch\n",
        "        netD.zero_grad()\n",
        "        # Format batch\n",
        "        real_cpu = data[0].to(device)\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "        # Forward pass real batch through D\n",
        "        output = netD(real_cpu).view(-1)\n",
        "        # Calculate loss on all-real batch\n",
        "        errD_real = criterion(output, label)\n",
        "        # Calculate gradients for D in backward pass\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        ## Train with all-fake batch\n",
        "        # Generate batch of latent vectors\n",
        "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "        # Generate fake image batch with G\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "        # Classify all fake batch with D\n",
        "        output = netD(fake.detach()).view(-1)\n",
        "        # Calculate D's loss on the all-fake batch\n",
        "        errD_fake = criterion(output, label)\n",
        "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        # Compute error of D as sum over the fake and the real batches\n",
        "        errD = errD_real + errD_fake\n",
        "        # Update D\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "        output = netD(fake).view(-1)\n",
        "        # Calculate G's loss based on this output\n",
        "        errG = criterion(output, label)\n",
        "        # Calculate gradients for G\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        # Update G\n",
        "        optimizerG.step()\n",
        "\n",
        "        # Output training stats\n",
        "        if i % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, num_epochs, i, len(train_dataloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "\n",
        "        # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(train_dataloader)-1)):\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise).detach().cpu()\n",
        "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "\n",
        "        # Log scalar values\n",
        "        writer.add_scalar('Loss/Discriminator', errD.item(), global_step=iters)\n",
        "        writer.add_scalar('Loss/Generator', errG.item(), global_step=iters)\n",
        "        writer.add_scalar('Performance/D(x)', D_x, global_step=iters)\n",
        "        writer.add_scalar('Performance/D(G(z1))', D_G_z1, global_step=iters)\n",
        "        writer.add_scalar('Performance/D(G(z2))', D_G_z2, global_step=iters)\n",
        "\n",
        "        # Log images generated by the GAN\n",
        "        if iters % 500 == 0 or ((epoch == num_epochs-1) and (i == len(train_dataloader)-1)):\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise).detach().cpu()\n",
        "            img_grid = vutils.make_grid(fake, padding=2, normalize=True)\n",
        "            writer.add_image('Generated Images', img_grid, global_step=iters)\n",
        "\n",
        "        iters += 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "0Si-zy6nLTGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWCY_KWGlvh9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKT0B_gvl5KC"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KagQ7VRJmFys"
      },
      "outputs": [],
      "source": [
        "# Grab a batch of real images from the dataloader\n",
        "real_batch = next(iter(train_dataloader))\n",
        "\n",
        "# Plot the real images\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,2,1)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Real Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
        "\n",
        "# Plot the fake images from the last epoch\n",
        "plt.subplot(1,2,2)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Fake Images\")\n",
        "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above output we can say that the fake images generated by the GAN are almost indistinguishable for some of the letters like \"Y\",'a',\"E\",\"S\",\"k\", etc."
      ],
      "metadata": {
        "id": "Ohovgmb-iI8r"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPOGnIsWXJLY"
      },
      "source": [
        "#Contributions\n",
        "\n",
        "Shayan Darian: 20%\n",
        "\n",
        "Sai Chaitanya Kilambi: 20%\n",
        "\n",
        "Bharti Moryani: 20%\n",
        "\n",
        "Yashvi Navadia: 20%\n",
        "\n",
        "Drew Williams: 20%\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}